{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary to display the plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "from IPython.display import Markdown as md\n",
    "from IPython.display import display, display_markdown, Markdown as md\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Helper functions defined in the helper.py file\n",
    "import helper as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use to reload the helper functions when the helper.py file is modified\n",
    "import importlib\n",
    "importlib.reload(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('https://fonts.googleapis.com/css2?family=DM+Sans:ital,opsz,wght@0,9..40,100..1000;1,9..40,100..1000&display=swap');\n",
       "div.text_cell {font-family : DM Sans, sans-serif !important;}\n",
       "pre {font-family : DM Sans, sans-serif !important;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "@import url('https://fonts.googleapis.com/css2?family=DM+Sans:ital,opsz,wght@0,9..40,100..1000;1,9..40,100..1000&display=swap');\n",
    "div.text_cell {font-family : DM Sans, sans-serif !important;}\n",
    "pre {font-family : DM Sans, sans-serif !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Clustering Model Comparison Template** \n",
    "This Notebook compares the performance of different types of clustering models on a dataset provided by the user. It assists in the model selection process by streamlining data cleaning, data preprocessing, model training, and model evaluation. Throughout the template there are sections the user must configure to match characteristics of their dataset. These sections are preceeded by tripple quote comments that direct the user on how to proceed. Users should store files containing their data in the data directory. The load_data function automatically prepends /data to the given file name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load and Reformat the Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Configure Data Loading Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input the name of the file containing your dataset, a list of the name of columns to drop, and a list of\n",
    "the representations of missing values in the dataset.\n",
    "\"\"\"\n",
    "data_file_name: str = \"\" \n",
    "columns_to_drop: list[str] = [\"\"] \n",
    "\n",
    "# Ensure no valid dataset values are included in this list\n",
    "dataset_na_value_representations = ['', 'NA', 'N/A', 'null', 'NULL', 'NaN', 'none', 'None', '-', '?', \"nan\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df: pd.DataFrame = hp.load_data(data_file_name, columns_to_drop, dataset_na_value_representations)\n",
    "\n",
    "initial_number_of_entries: int = len(data_df) \n",
    "variable_list: list[str] = list(data_df.columns)\n",
    "number_of_variables: int = len(variable_list)\n",
    "\n",
    "numerical_variables: list[str] = list(data_df.select_dtypes(include = np.number).columns)\n",
    "categorical_variables: list[str] = list(data_df.select_dtypes(exclude = np.number).columns)\n",
    "\n",
    "# Sets the valid categories for each categorical variable. Set show_categories to True to display the categories\n",
    "unique_categories_dict: dict[str, list[str]] = hp.get_categories(data_df, categorical_variables, show_categories = False)\n",
    "    \n",
    "for variable, variable_categories in unique_categories_dict.items():\n",
    "    data_df[variable] = pd.Categorical(data_df[variable], categories = variable_categories)\n",
    "\n",
    "hp.display_text(f\"Numerical Variables: {numerical_variables}\", font_size = 16)\n",
    "hp.display_text(f\"Categorical Variables: {categorical_variables}\", font_size = 16)\n",
    "print()\n",
    "\n",
    "hp.display_df(data_df.head(), font_size = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reformat Columns**\n",
    "Do not run this cell if the values of your dataset are already properly formatted. If not (e.g. columns that should be numerical are instead represented as strings), open the cell and configure this section to reformat any imporperly formatted columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Apply value cleaning/conversion functions to the relevant columns of your datset using the apply \n",
    "method on your DataFrame. Your cleaing/conversion functions should take in a single string and return a float.\n",
    "Pass them into the apply method as an argument without parentheses.\n",
    "\n",
    "Example: data_df['column_name'] = data_df['column_name'].apply(clean_function).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "hp.display_text(f\"Previous List of Numerical Variables: {numerical_variables}\", font_size = 18)\n",
    "hp.display_text(f\"Previous List of Categorical Variables: {categorical_variables}\", font_size = 18)\n",
    "print()\n",
    "\n",
    "numerical_variables: list[str] = list(data_df.select_dtypes(include = np.number).columns)\n",
    "categorical_variables: list[str] = list(data_df.select_dtypes(exclude = np.number).columns)\n",
    "\n",
    "unique_categories_dict: dict[str, list[str]] = hp.get_categories(data_df, categorical_variables, show_category_count = False, show_categories = False)\n",
    "for variable, variable_categories in unique_categories_dict.items():\n",
    "    data_df[variable] = pd.Categorical(data_df[variable], categories = variable_categories)\n",
    "\n",
    "hp.display_text(f\"Updated List of Numerical Variables: {numerical_variables}\", font_size = 18)\n",
    "hp.display_text(f\"Updated List of Categorical Variables: {categorical_variables}\", font_size = 18)\n",
    "\n",
    "hp.display_df(data_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handle Missing Values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Initial Data Profiling**\n",
    "The following cell uses ydata-profiling to generate a detailed report on the characteristics of the input dataset. Use this information to help you determine how to handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_dataset_report = ProfileReport(data_df, title = \"Dataset Profiling Report (Before Handling Outliers/Missing Values)\", progress_bar = False, explorative = True)\n",
    "initial_dataset_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Information on Missing Values in Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns_with_missing_values: list[str] = data_df[numerical_variables].columns[data_df[numerical_variables].isnull().any()].tolist()\n",
    "categorical_columns_with_missing_values: list[str] = data_df[categorical_variables].columns[data_df[categorical_variables].isnull().any()].tolist()\n",
    "all_columns_with_missing_values: list[str] = numerical_columns_with_missing_values + categorical_columns_with_missing_values\n",
    "\n",
    "if len(all_columns_with_missing_values) != 0:\n",
    "    print()\n",
    "    entries_with_missing_values_df: pd.DataFrame = data_df[all_columns_with_missing_values][data_df[all_columns_with_missing_values].isnull().any(axis = \"columns\")]\n",
    "    number_of_entries_with_missing_values: int = len(entries_with_missing_values_df)\n",
    "    percent_of_entries_with_missing_values: float = (number_of_entries_with_missing_values / initial_number_of_entries) * 100  \n",
    "    \n",
    "    hp.display_text(f\"Total Number of Entries: {initial_number_of_entries}\")\n",
    "    hp.display_text(f\"Total Number of Entreis with at Least One Missing Value: {number_of_entries_with_missing_values} ({percent_of_entries_with_missing_values:.2f}% of Entries)\")\n",
    "    hp.display_text(f\"Number of Entries if all Rows with Missing Values are Dropped: {initial_number_of_entries - number_of_entries_with_missing_values}\")\n",
    "    print()\n",
    "    hp.display_text(\"Up to First 5 Entries with Missing Values:\")\n",
    "    hp.display_df(entries_with_missing_values_df.head(), font_size = 16)\n",
    "else:\n",
    "    print()\n",
    "    hp.display_text(\"No Missing Values in Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Drop or Impute Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Specify how you would like to handle missing values in the dataset. All rows with missing data are dropped by default. Will it work\n",
    "\"\"\"\n",
    "data_df = hp.drop_rows_with_missing_values(data_df, all_columns_with_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handle Outliers/Eronious Entries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Implement Outlier Handling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provides information on variable distributions to help users determine whether they should drop outlier entries\n",
    "hp.display_text(\"Previous Numerical Variable Statistics\", font_size = 20)\n",
    "hp.display_df(data_df.describe(), 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use the visualize_outliers function to identify and optionally remove outliers in the numerical columns of your dataset.\n",
    "\"\"\"\n",
    "data_df = hp.visualize_outliers(data_df, numerical_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.display_text(\"Updated Numerical Variable Statistics\", font_size = 20)\n",
    "hp.display_df(data_df.describe(), 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preprocessing**\n",
    "Define your preprocessing steps within this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset Preprocessing Information**\n",
    "Use the information provided by the comparison profile report to analyze the result of your data cleaning and to help determine your preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_report = ProfileReport(data_df, title = \"Dataset Profiling Report (After Handling Outliers/Missing Values)\", progress_bar = False, explorative = True)\n",
    "# The difference between the profiling report before and after preprocessing may not be very significant depending on the number of missing values\n",
    "# and removed outliers\n",
    "post_cleaning_comparison_report = dataset_report.compare(initial_dataset_report)\n",
    "\n",
    "#post_cleaning_comparison_report\n",
    "display(dataset_report)\n",
    "\n",
    "hp.display_text(f\"Numerical Variables: {numerical_variables}\", font_size = 16)\n",
    "hp.display_text(f\"Categorical Variables: {categorical_variables}\", font_size = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Configure Preprocessing Steps for the Provided General Preprocessor**\n",
    "The provided general preprocessor address three common preprocessing transformations: scaling numerical variables, one-hot encoding nominal categorical variables, and ordianal encoding ordinal categorical variables. Customize these steps to fit the needs of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input a list of the numerical variables you would like to scale (defaultes to all numerical\n",
    "variables in the dataset) and a list of the nominal categorical variables you would like to one-hot encode.\n",
    "\"\"\"\n",
    "numerical_variables_to_scale: list[str] = numerical_variables\n",
    "nominal_categorical_variables_to_encode: list[str] = [\"\"]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The ordianl_categories_ordered_dict variable represents the order of ordinal categorical variable categories in a dictionary.\n",
    "For the keys of this dictionary, input the column names of ordinal categorical variables. Each key's value should be a lists\n",
    "of variable categories ordered from \"smallest\" to \"largest\". Example:\n",
    "ordianl_categories_ordered_dict = {\n",
    "    \"size\": [\"small\", \"medium\", \"large\"],\n",
    "    \"grade\": [\"F\", \"D\", \"C\", \"B\", \"A]\n",
    "}\n",
    "\"\"\"\n",
    "ordianl_categories_ordered_dict: dict[str, list[str]] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **General Preprocessor Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the keys and values of the ordianl_categories_ordered_dict into separate lists\n",
    "ordianl_variable_categories_to_encode: list[str] = list(ordianl_categories_ordered_dict.keys())\n",
    "ordianl_variable_categories_orders_lists: list[list[str]] = list(ordianl_categories_ordered_dict.values())\n",
    "\n",
    "# The argumnet for the transformers parameter of ColumnTransformer must be a a list of touples with three entries. Each of these touples\n",
    "# represents a preprocessing step. The first entry of each touple is a name for the step. The second entry is the transformer object, and\n",
    "# the final entry is a list of the columns the step should be applied to.\n",
    "general_variable_preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('numerical_scaler', StandardScaler(), numerical_variables_to_scale),\n",
    "        ('nominal_encoder', OneHotEncoder(drop = \"first\", handle_unknown = 'ignore'), nominal_categorical_variables_to_encode),\n",
    "        (\"ordinal_encoder\", OrdinalEncoder(categories = ordianl_variable_categories_orders_lists), ordianl_variable_categories_to_encode)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocessing Results**\n",
    "Use the displayed information to confirm that your preprocessing steps have been applied as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.display_text(f\"Scaled Numerical Variables: {numerical_variables_to_scale}\")\n",
    "hp.display_text(f\"Encoded Nominal Categorical Variables: {nominal_categorical_variables_to_encode}\")\n",
    "hp.display_text(f\"Encoded Ordinal Categorical Variables (confirm that category orders were assigned to the correct ordinal categorical variable):\")\n",
    "\n",
    "if len(ordianl_variable_categories_to_encode) != 0:\n",
    "    for i in range(len(ordianl_variable_categories_to_encode)):\n",
    "        display_markdown(md(f\"* {ordianl_variable_categories_to_encode[i]}: {ordianl_variable_categories_orders_lists[i]}\"))\n",
    "else:\n",
    "    hp.display_text(\"None\")\n",
    "    \n",
    "print()\n",
    "hp.display_df(data_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **K-Means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configure the values of k you would like to test for the KMeans clustering algorithm (Default is 2 through 10).\n",
    "\"\"\"\n",
    "k_values = range(2, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Assess K-Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_custom_metrics(model):\n",
    "    return {'inertia': model.inertia_}\n",
    "\n",
    "kmeans_model_name: str = \"KMeans\"\n",
    "kmeans_instances: list = [KMeans(n_clusters = k, random_state = 42) for k in k_values]\n",
    "kmeans_pipelines: list = hp.create_model_pipelines(kmeans_instances, general_variable_preprocessor)\n",
    "kmeans_results: dict = hp.fit_clustering_models(data_df, kmeans_pipelines, kmeans_model_name, custom_metric_func = kmeans_custom_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.plot_kmeans_analysis(kmeans_results, k_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Select a K-Value to Further Analyze**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set the k value for the clustering model you would like to analyze in-depth (default is 7).\n",
    "\"\"\"\n",
    "k = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_selected_model = kmeans_results[k][\"model\"]\n",
    "\n",
    "hp.plot_cluster_sizes(kmeans_results, k)\n",
    "kmeans_feature_importance_df = hp.plot_feature_importance_heatmap(kmeans_results, k, data_df)\n",
    "hp.plot_feature_distributions(kmeans_results, k, data_df)\n",
    "hp.plot_cluster_kde(kmeans_results, k, data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generate Profile Reports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_cluster_reports: dict[int, tuple[ProfileReport, ProfileReport]] = hp.analyze_clusters_with_profiling(data_df, kmeans_results, k, dataset_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Choose the profiling reports you would like to display\n",
    "\"\"\"\n",
    "cluster_5_report, cluster_5_comparison = kmeans_cluster_reports[5]\n",
    "cluster_5_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Save Best Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configure the number of clusters of the model you want to save (current k by default), the name of the file\n",
    "that the model should be saved to, and the save method.\n",
    "\"\"\"\n",
    "num_clusters_to_save = k\n",
    "save_file_name: str = \"\"\n",
    "save_method: str = \"\" # Options: \"pickle\" or \"joblib\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Save Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.save_model(save_file_name, num_clusters_to_save, kmeans_results, save_method)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clustering-template",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
